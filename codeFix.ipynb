{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from venv import create\n",
    "from django.shortcuts import render\n",
    "from django.http import HttpResponse\n",
    "from matplotlib.style import context\n",
    "\n",
    "import spacy\n",
    "from yaml import load\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict\n",
    "\n",
    "from NewsToUserPersona.createUserPersona.jobTitlesList import job_titles\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Create your views here.\n",
    "def index(request):\n",
    "    return render(request, 'createUserPersona/index.html')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kelas user persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kelas untuk object userpersona\n",
    "class UserPersona:\n",
    "    def __init__(self, name, work=\"\", job_title=\"\", goals=[]):\n",
    "        self.name = name\n",
    "        self.work = work\n",
    "        self.goals = goals\n",
    "        self.job_title = job_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing dengan spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing dengan spacy\n",
    "def preprocessingWithSpacy(input):\n",
    "    doc = nlp(input)\n",
    "\n",
    "    # menggunakan lemmatized form\n",
    "    # lemmatized_words = \" \".join([token.lemma_ for token in doc])\n",
    "    # doc = nlp(lemmatized_words)\n",
    "\n",
    "    # mencari pattern goals\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'POS': 'VERB'},\n",
    "            {'POS': 'ADJ'},\n",
    "            {'POS': 'NOUN'}]\n",
    "    matcher.add(\"HelloWorld\", [pattern])\n",
    "    \n",
    "    filtered_goals = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "        filtered_goals.append(span.text)\n",
    "\n",
    "    # mencari kalimat-kalimat yang mengandung goals\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    docs_for_pattern_goals = [nlp.make_doc(text) for text in filtered_goals]\n",
    "    matcher.add(\"MencariKalimatMengandungGoals\", docs_for_pattern_goals)\n",
    "\n",
    "    filtered_sentences_containing_goals = []\n",
    "    for sent in doc.sents:\n",
    "        for match_id, start, end in matcher(nlp(sent.text)):\n",
    "            if nlp.vocab.strings[match_id] in [\"MencariKalimatMengandungGoals\"]:\n",
    "                filtered_sentences_containing_goals.append(sent.text)\n",
    "\n",
    "    filtered_sentences_containing_goals = np.unique(filtered_sentences_containing_goals)\n",
    "    a = 1\n",
    "    for i in filtered_sentences_containing_goals:\n",
    "        print(a, i)\n",
    "        a=a+1\n",
    "    \n",
    "    return filtered_sentences_containing_goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function untuk membuat user persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function untuk membuat user persona\n",
    "def createUserPersona(group_of_sentences):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    userPersonaCreated = []\n",
    "    i = 1\n",
    "    no_entities = []\n",
    "\n",
    "    for each in group_of_sentences:\n",
    "        \n",
    "        go = nlp(str(each))\n",
    "\n",
    "        token_types = [token.ent_type_ for token in go]\n",
    "        \n",
    "        pattern = [{'POS': 'VERB'},\n",
    "            {'POS': 'ADJ'},\n",
    "            {'POS': 'NOUN'}]\n",
    "        matcher.add(\"EndResult\", [pattern])\n",
    "        \n",
    "        matches3 = matcher(go)\n",
    "\n",
    "        nomor_goals = 1\n",
    "            \n",
    "        # memilah apakah dia memiliki entitas, jika ya, akan langsung dicetak, jika tidak maka akan dimasukkan dalam array\n",
    "        if('PERSON' in token_types) or ('ORG' in token_types):\n",
    "            print(\"====== USER PERSONA \", i, \"============\")\n",
    "\n",
    "            # mencari nama\n",
    "            for ent in go.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    nama = ent.text\n",
    "                    print('Name:' + ent.text)\n",
    "                    break\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    nama = ent.text\n",
    "                    print('Name:' + ent.text)\n",
    "                    break     \n",
    "            \n",
    "            # mencari organisasi\n",
    "            if('PERSON' in token_types) and ('ORG' in token_types):\n",
    "                for ent in go.ents:\n",
    "                    if ent.label_== \"ORG\":\n",
    "                        kerja = ent.text\n",
    "                        print('Work:' + ent.text)                    \n",
    "            else:\n",
    "                kerja = \"\"\n",
    "                print('Work: N/A')\n",
    "            print()\n",
    "\n",
    "            # mencari job title\n",
    "            job_matcher = PhraseMatcher(nlp.vocab)\n",
    "            jabatan = job_titles\n",
    "\n",
    "            patterns_job = [nlp.make_doc(text) for text in jabatan]\n",
    "            job_matcher.add(\"TerminologyList\", patterns_job)\n",
    "\n",
    "            nama_jabatan = \"\"\n",
    "            job_titles_found = job_matcher(go)\n",
    "            for match_id, start, end in job_titles_found:\n",
    "                nama_jabatan = go[start:end]\n",
    "                print(go[start:end])\n",
    "\n",
    "            goals_found = []\n",
    "            for match_id, start, end in matches3:\n",
    "                string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                span = go[start:end]  # The matched span\n",
    "                goals_found.append(span.text)\n",
    "                print(nomor_goals, span.text)\n",
    "                nomor_goals = nomor_goals+1   \n",
    "            print(\"===========================\")\n",
    "\n",
    "            ketemu = UserPersona(nama, kerja, str(nama_jabatan), goals_found)\n",
    "            userPersonaCreated.append(ketemu.__dict__)\n",
    "\n",
    "        else:\n",
    "            for match_id, start, end in matches3:\n",
    "                string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "                span = go[start:end]  # The matched span\n",
    "                no_entities.append(span.text)\n",
    "            continue\n",
    "        \n",
    "        i = i+1\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(\"====== USER PERSONA \", i, \"============\")\n",
    "    print('User')\n",
    "    nomor_no_entities = 1\n",
    "    for each in no_entities:\n",
    "        print(nomor_no_entities, each)\n",
    "        nomor_no_entities = nomor_no_entities+1\n",
    "    ketemu = UserPersona('User', 'N/A', 'N/A', no_entities)\n",
    "    userPersonaCreated.append(ketemu.__dict__)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"===========================\")\n",
    "    json_hasil = json.dumps(userPersonaCreated)\n",
    "\n",
    "    return json_hasil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mencari kalimat yang ber entitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mencari kalimat yang ber entitas\n",
    "def findingSentencesWithEntities(input):\n",
    "    doc = nlp(input)\n",
    "\n",
    "    who_aspect = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" or ent.label_ == \"ORG\":\n",
    "            who_aspect.append(ent.text)\n",
    "\n",
    "    who_aspect = np.unique(who_aspect)\n",
    "\n",
    "    print(who_aspect)\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    terms = who_aspect.tolist()\n",
    "\n",
    "    # Only run nlp.make_doc to speed things up\n",
    "    patterns = [nlp.make_doc(text) for text in terms]\n",
    "    matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "    filtered_sentences = []\n",
    "    for sent in doc.sents:\n",
    "        for match_id, start, end in matcher(nlp(sent.text)):\n",
    "            if nlp.vocab.strings[match_id] in [\"TerminologyList\"]:\n",
    "                filtered_sentences.append(sent.text)\n",
    "\n",
    "    filtered_sentences = np.unique(filtered_sentences)\n",
    "\n",
    "    print(filtered_sentences)\n",
    "\n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling entitas\n",
    "def profilingEntities(sentences):\n",
    "    list_of_user_persona = []\n",
    "    list_of_names = []\n",
    "    \n",
    "    for each in sentences:\n",
    "        go = nlp((str(each)))\n",
    "        token_types = [token.ent_type_ for token in go]\n",
    "\n",
    "        if('PERSON' in token_types) or ('ORG' in token_types):\n",
    "\n",
    "            # mencari nama\n",
    "            for ent in go.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    nama = ent.text\n",
    "                    print('Person:' + ent.text)\n",
    "                    break\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    nama = ent.text\n",
    "                    print('Organization:' + ent.text)\n",
    "                    pass     \n",
    "            \n",
    "            # mencari organisasi\n",
    "            if('PERSON' in token_types) and ('ORG' in token_types):\n",
    "                for ent in go.ents:\n",
    "                    if ent.label_== \"ORG\":\n",
    "                        kerja = ent.text\n",
    "                        print('Work:' + ent.text)\n",
    "                    \n",
    "            else:\n",
    "                kerja = \"\"\n",
    "                print('Work: N/A')\n",
    "            print()\n",
    "\n",
    "            # mencari job title\n",
    "            job_matcher = PhraseMatcher(nlp.vocab)\n",
    "            jabatan = job_titles\n",
    "\n",
    "            patterns_job = [nlp.make_doc(text) for text in jabatan]\n",
    "            job_matcher.add(\"TerminologyList\", patterns_job)\n",
    "\n",
    "            nama_jabatan = \"\"\n",
    "            job_titles_found = job_matcher(go)\n",
    "            for match_id, start, end in job_titles_found:\n",
    "                nama_jabatan = go[start:end]\n",
    "                print(go[start:end])\n",
    "\n",
    "            baru = UserPersona(nama, kerja, str(nama_jabatan), [])\n",
    "            list_of_user_persona.append(baru.__dict__)\n",
    "        \n",
    "        res_list = []\n",
    "        for i in range(len(list_of_user_persona)):\n",
    "            if list_of_user_persona[i] not in list_of_user_persona[i + 1:]:\n",
    "                res_list.append(list_of_user_persona[i])\n",
    "        print('res list', res_list)\n",
    "        json_hasil = json.dumps(res_list)\n",
    "\n",
    "    return json_hasil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mengggabungkan hasil dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(d1, d2):\n",
    "    dd = defaultdict(list)\n",
    "\n",
    "    for d in (d1, d2):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, list):\n",
    "                dd[key].extend(value)\n",
    "            else:\n",
    "                dd[key].append(value)\n",
    "    return dict(dd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "menggabungkan dan menyortir persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function untuk menggabungkan dictionary dan mensortir persona\n",
    "def MergeAndSort(baca_json):\n",
    "    name_sementara = []\n",
    "    for each in baca_json:\n",
    "        name_sementara.append(each['name'])\n",
    "\n",
    "    name_sementara = np.unique(name_sementara)\n",
    "\n",
    "    from operator import itemgetter\n",
    "    baca_json = sorted(baca_json, key=itemgetter('name', 'goals'), reverse=True) \n",
    "\n",
    "    temp = []\n",
    "    temp_nama = None\n",
    "    temp_kerja = None\n",
    "    temp_organisasi = None\n",
    "\n",
    "    Sorted_Filtered_Persona = []\n",
    "\n",
    "    for nama in name_sementara:\n",
    "        for each in baca_json:\n",
    "            if each['name'] == nama:\n",
    "                if each['goals']:\n",
    "                    temp.extend(each['goals'])\n",
    "\n",
    "                if each['job_title']:\n",
    "                    temp_kerja = each['job_title']\n",
    "\n",
    "                if each['work']:\n",
    "                    temp_organisasi = each['work']\n",
    "\n",
    "                temp_nama = nama\n",
    "                \n",
    "            else:\n",
    "                if len(temp) > 1:\n",
    "                    print(\"hi\")\n",
    "\n",
    "                if temp_nama:\n",
    "                    Sorted_Filtered_Persona.append(UserPersona(temp_nama, temp_organisasi, temp_kerja, temp).__dict__)\n",
    "                    print(temp_nama, temp_kerja, temp_organisasi, temp)\n",
    "                    \n",
    "                temp = []\n",
    "                temp_nama = [] \n",
    "                temp_kerja = []\n",
    "                temp_organisasi = []\n",
    "\n",
    "\n",
    "    return Sorted_Filtered_Persona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mencari individual item yang ada dalam persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemsFound(listPersonaHasilMerge):\n",
    "    foundNames = []\n",
    "    foundJob = []\n",
    "    foundWork = []\n",
    "    foundGoals = []\n",
    "    for each in listPersonaHasilMerge:\n",
    "        if each['name'] != 'User':\n",
    "            foundNames.append(each['name'])\n",
    "        if each['job_title'] != 'N/A' and each['job_title'] != []:\n",
    "            foundJob.append(str(each['job_title']))\n",
    "        if each['work'] != 'N/A' and each['work'] != []:\n",
    "            foundWork.append(str(each['work']))\n",
    "        for goal in each['goals']:\n",
    "            foundGoals.append(goal)\n",
    "    print(foundJob)\n",
    "\n",
    "    foundJob = np.unique(foundJob).tolist()\n",
    "    foundWork = np.unique(foundWork).tolist()\n",
    "\n",
    "    jumlah_nama = len(foundNames)\n",
    "    jumlah_job = len(foundJob)\n",
    "    jumlah_work = len(foundWork)\n",
    "    jumlah_goals = len(foundGoals)\n",
    "\n",
    "\n",
    "    dictionary_item = {'names': foundNames, 'jobs': foundJob, 'works': foundWork, 'goals': foundGoals, 'jumlah_nama': jumlah_nama, 'jumlah_job': jumlah_job, 'jumlah_work': jumlah_work, 'jumlah_goals': jumlah_goals}\n",
    "\n",
    "    return dictionary_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kode untuk menjalankan semua function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letsGo(request):\n",
    "    konten = request\n",
    "    \n",
    "    preprocessed_konten = preprocessingWithSpacy(konten)\n",
    "    created_persona = createUserPersona(preprocessed_konten)\n",
    "\n",
    "    sentences_with_entities = findingSentencesWithEntities(konten)\n",
    "    potential_entities = profilingEntities(sentences_with_entities)\n",
    "\n",
    "    created_persona_dict = json.loads(created_persona)\n",
    "    potential_entities_dict = json.loads(potential_entities)\n",
    "\n",
    "    print(potential_entities)\n",
    "\n",
    "    combined_user_persona = created_persona_dict + potential_entities_dict\n",
    "    print(combined_user_persona)\n",
    "    combined_user_persona = MergeAndSort(combined_user_persona)\n",
    "    print(combined_user_persona)\n",
    "    individualItems = itemsFound(combined_user_persona)\n",
    "    print('INDIVIDUAL ITEMS', individualItems)\n",
    "    \n",
    "\n",
    "\n",
    "    send_to_render = {'user_persona': combined_user_persona, 'individual_item': individualItems, 'konten': konten}\n",
    "\n",
    "\n",
    "    \n",
    "    # return HttpResponse(send_to_render)\n",
    "    return render(request, 'createUserPersona/result.html', send_to_render)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
